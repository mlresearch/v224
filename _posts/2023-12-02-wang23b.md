---
title: Cost-Effective Hyperparameter Optimization for Large Language Model Generation
  Inference
openreview: DoGmh8A39O
abstract: 'Large Language Models (LLMs) have sparked significant interest in their
  generative capabilities, leading to the development of various commercial applications.
  The high cost of using the models drives application builders to maximize the value
  of generation under a limited inference budget. This paper presents a study of optimizing
  inference hyperparameters such as the number of responses, temperature and max tokens,
  which significantly affects the utility/cost of text generation. We design a framework
  named EcoOptiGen which leverages economical hyperparameter optimization and cost-based
  pruning. Experiments with the GPT-3.5/GPT-4 models on a variety of tasks verify
  its effectiveness. EcoOptiGen is implemented in the “autogen” package of the FLAML
  library: \url{https://aka.ms/autogen}.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23b
month: 0
tex_title: Cost-Effective Hyperparameter Optimization for Large Language Model Generation
  Inference
firstpage: 30/1
lastpage: 17
page: 30/1-17
order: 30
cycles: false
bibtex_author: Wang, Chi and Liu, Xueqing and Awadallah, Ahmed Hassan
author:
- given: Chi
  family: Wang
- given: Xueqing
  family: Liu
- given: Ahmed Hassan
  family: Awadallah
date: 2023-12-02
address:
container-title: Proceedings of the Second International Conference on Automated Machine
  Learning
volume: '228'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v228/wang23b/wang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
