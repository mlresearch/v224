---
title: Optimal Resource Allocation for Early Stopping-based Neural Architecture Search
  Methods
openreview: lmtNt--6dw
abstract: The field of NAS has been significantly benefiting from the increased availability
  of parallel compute resources, as optimization algorithms typically require sampling
  and evaluating hundreds of model configurations. Consequently, to make use of these
  resources, the most commonly used early stopping-based NAS methods are suitable
  for running multiple trials in parallel. At the same time, also the training time
  of single model configurations can be reduced, e.g., by employing data-parallel
  training using multiple GPUs.   This paper investigates the optimal allocation of
  a fixed amount of parallel workers for conducting NAS. In practice, users have to
  decide if the computational resources are primarily used to assign more workers
  to the training of individual trials or to increase the number of trials executed
  in parallel. The first option accelerates the speed of the individual trials (exploitation)
  but reduces the parallelism of the NAS loop, whereas for the second option, the
  runtime of the trials is longer but a larger number of simultaneously processed
  trials in the NAS loop is achieved (exploration).   Our study encompasses both large-
  and small-scale scenarios, including tuning models in parallel on a single GPU,
  with data-parallel training on up to 16 GPUs, and measuring the scalability of NAS
  on up to 64 GPUs. Our empirical results using the HyperBand, Asynchronous Successive
  Halving, and Bayesian Optimization HyperBand methods offer valuable insights for
  users seeking to run NAS on both small and large computational budgets. By selecting
  the appropriate number of parallel evaluations, the NAS process can be accelerated
  by factors of ${\approx}$2â€“5 while preserving the test set accuracy compared to
  non-optimal resource allocations.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aach23a
month: 0
tex_title: Optimal Resource Allocation for Early Stopping-based Neural Architecture
  Search Methods
firstpage: 12/1
lastpage: 17
page: 12/1-17
order: 12
cycles: false
bibtex_author: Aach, Marcel and Inanc, Eray and Sarma, Rakesh and Riedel, Morris and
  Lintermann, Andreas
author:
- given: Marcel
  family: Aach
- given: Eray
  family: Inanc
- given: Rakesh
  family: Sarma
- given: Morris
  family: Riedel
- given: Andreas
  family: Lintermann
date: 2023-12-02
address:
container-title: Proceedings of the Second International Conference on Automated Machine
  Learning
volume: '224'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 12
  - 2
pdf: https://proceedings.mlr.press/v224/aach23a/aach23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
